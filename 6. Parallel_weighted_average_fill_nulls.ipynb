{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the 6th notebook\n",
    "- To fill null by distance weighted average with multi process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data maniuplation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "# calculate distance from GCS data (latitude and longitude)\n",
    "from geopy.distance import geodesic\n",
    "# multi process\n",
    "from multiprocessing import Pool\n",
    "# Import the function inside .py\n",
    "from fill_nulls_multiprocess import process_date_subset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv that mapped region to district\n",
    "df = pd.read_csv(\"data_gcs_combine_update_district.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_distances_with_geopy(stations, threshold):\n",
    "    \"\"\"\n",
    "    Precompute the pairwise distances and weights (1/distance^2) between stations,\n",
    "    filter pairs based on the distance threshold, and remove self-pairs.\n",
    "\n",
    "    Parameters:\n",
    "        stations (pd.DataFrame): DataFrame containing 'region', 'latitude', and 'longitude'.\n",
    "        threshold (float): Distance threshold in kilometers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered distance matrix with columns ['station1', 'station2', 'distance', 'weight'].\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Create all station pairs\n",
    "    station_pairs = pd.DataFrame(list(product(stations['region'], repeat=2)), columns=['station1', 'station2'])\n",
    "\n",
    "    # Remove self-pairs\n",
    "    station_pairs = station_pairs[station_pairs['station1'] != station_pairs['station2']]\n",
    "\n",
    "    # Create a mapping for coordinates\n",
    "    station_coords = stations.set_index('region')[['latitude', 'longitude']]\n",
    "\n",
    "    # Calculate geodesic distances\n",
    "    station_pairs['distance'] = station_pairs.apply(\n",
    "        lambda row: geodesic(\n",
    "            station_coords.loc[row['station1']],\n",
    "            station_coords.loc[row['station2']]\n",
    "        ).km, axis=1\n",
    "    )\n",
    "\n",
    "    # Filter pairs by the distance threshold\n",
    "    station_pairs = station_pairs[station_pairs['distance'] <= threshold]\n",
    "\n",
    "    # Calculate weights (1/distance^2)\n",
    "    station_pairs['weight'] = station_pairs['distance'].apply(lambda d: 1 / (d ** 2))\n",
    "\n",
    "    return station_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fill_nulls(df, fields, distance_matrix, n_processes=4):\n",
    "    \"\"\"\n",
    "    Fill null values using multiprocessing by splitting the DataFrame by date.\n",
    "    \"\"\"\n",
    "    df_filled = df.copy()\n",
    "    dates = df_filled['date'].unique()\n",
    "\n",
    "    # Prepare arguments for multiprocessing\n",
    "    args = [(date, df_filled[df_filled['date'] == date], fields, distance_matrix) for date in dates]\n",
    "\n",
    "    # Use multiprocessing\n",
    "    with Pool(n_processes) as pool:\n",
    "        results = pool.map(process_date_subset, args)\n",
    "\n",
    "    # Combine results\n",
    "    df_filled = pd.concat(results, ignore_index=True)\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique row of region, latitude, longitude\n",
    "stations = df[['region', 'latitude', 'longitude']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the series to fill (exclude if the series is available in one station only )\n",
    "fields_to_fill = [\n",
    "    'Mean Pressure (hPa)', \n",
    "    'Total Rainfall (mm)', \n",
    "    'Mean Relative Humidity (%)', \n",
    "    'Maximum Temperature (째C)', \n",
    "    'Minimum Temperature (째C)', \n",
    "    'Mean Temperature (째C)', \n",
    "    'Prevailing Wind Direction (째)', \n",
    "    'Mean Wind Speed (km/h)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a distance threshold\n",
    "threshold = 30  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get helper dataframe with the weighting after calculated the 1/distanct^2 and passed threshold\n",
    "distance_matrix = precompute_distances_with_geopy(stations, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null with multi process \n",
    "filled_df = parallel_fill_nulls(df, fields_to_fill, distance_matrix, n_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it in csv\n",
    "filled_df.to_csv(\"data_gcs_combine_update_district_fillna.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
